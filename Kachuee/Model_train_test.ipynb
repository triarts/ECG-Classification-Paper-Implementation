{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paper Implementation\n",
    "Description: This is implementation of paper from M. Kachuee et al. \"ECG Heartbeat Classification: A Deep Transferable Representation\" [(source)](https://arxiv.org/abs/1805.00794)\n",
    "* Dataset from kaggle: [(source)](https://www.kaggle.com/shayanfazeli/heartbeat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO next\n",
    "- [x] implementation using dataset from kaggle (MITDB)\n",
    "- [ ] implementation transfer learning for PTB (MI disease) dataset\n",
    "- [ ] implementation using MITBIH dataset (selfmade implementation preprocessing method based on paper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\";\n",
    "# The GPU id to use, usually either \"0\" or \"1\";\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\";  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "#import re\n",
    "import matplotlib.pyplot as plt\n",
    "#import gc\n",
    "import pickle\n",
    "import pathlib\n",
    "import io\n",
    "from math import sqrt\n",
    "nrs = 1895\n",
    "np.random.seed(nrs)\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint,EarlyStopping,Callback,LearningRateScheduler\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten, Convolution1D, MaxPooling1D, GlobalMaxPool1D\n",
    "from tensorflow.keras.layers import Conv1D, MaxPool1D,Add #cnn\n",
    "from tensorflow.keras.layers import concatenate,Input\n",
    "from tensorflow.keras import optimizers, losses, activations\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.activations import relu\n",
    "\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "print(tf.__version__)\n",
    "\n",
    "#from sklearn.metrics import f1_score, accuracy_score\n",
    "#from sklearn.model_selection import train_test_split\n",
    "from keras.utils import to_categorical\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.utils.multiclass import unique_labels\n",
    "from collections import Counter\n",
    "\n",
    "from calcCM import *\n",
    "\n",
    "# print(gc.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## limit GPU memory usage\n",
    "configproto = tf.compat.v1.ConfigProto()\n",
    "configproto.gpu_options.per_process_gpu_memory_fraction = 0.3\n",
    "configproto.gpu_options.allow_growth = True\n",
    "\n",
    "#limit CPU logical core usage\n",
    "configproto.intra_op_parallelism_threads=2\n",
    "configproto.inter_op_parallelism_threads=2\n",
    "\n",
    "#setup session config\n",
    "sess = tf.compat.v1.Session(config=configproto)\n",
    "tf.compat.v1.keras.backend.set_session(sess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### set up result folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get current path directory\n",
    "dirPath = os.path.abspath(os.getcwd())\n",
    "print(dirPath)\n",
    "#print(pathlib.Path().absolute())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create uniq folder name for results\n",
    "now = datetime.now()\n",
    "\n",
    "dt_string_daytime = now.strftime(\"%Y%m%d_%H%M%S\")\n",
    "dt_string_daytime += '_kachueeCNN-singleRun'\n",
    "print(\"folder name =\", dt_string_daytime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create folder for results\n",
    "data_result_path = list()\n",
    "data_result_path.append('model_'+dt_string_daytime)\n",
    "data_result_path.append(data_result_path[0]+'/ckpt')\n",
    "data_result_path.append(data_result_path[0]+'/fitHistory')\n",
    "data_result_path.append(data_result_path[0]+'/modelSummary')\n",
    "\n",
    "for i in range(0,len(data_result_path)):\n",
    "    if not os.path.exists(data_result_path[i]):\n",
    "        print(\"create new folder \"+data_result_path[i])\n",
    "        os.makedirs(data_result_path[i])\n",
    "    else:\n",
    "        print(data_result_path[i]+\" already created\")\n",
    "\n",
    "currentPath = dirPath+'/'+data_result_path[0]\n",
    "print(currentPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def activation_func(pos):\n",
    "    if(pos== 1):\n",
    "        return activations.relu\n",
    "    elif(pos==2):\n",
    "        return activations.sigmoid\n",
    "    elif(pos==3):\n",
    "        return activations.softmax\n",
    "    elif(pos==4):\n",
    "        return activations.tanh\n",
    "    else:\n",
    "        return activations.relu\n",
    "    \n",
    "def opt_func(pos,lr=0.0001):\n",
    "    if(pos==1):\n",
    "        return optimizers.Adam(learning_rate=lr, beta_1=0.9, beta_2=0.999, amsgrad=False)\n",
    "    elif(pos==2):\n",
    "        return optimizers.Adagrad(learning_rate=lr,initial_accumulator_value=0.8)\n",
    "    elif(pos==3):\n",
    "        return optimizers.Nadam(learning_rate=lr, beta_1=0.9, beta_2=0.999)\n",
    "    elif(pos==4):\n",
    "        return optimizers.Adamax(learning_rate=lr, beta_1=0.9, beta_2=0.999)\n",
    "    else:\n",
    "        return optimizers.Adam(learning_rate=lr, beta_1=0.9, beta_2=0.999, amsgrad=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training parameter setup\n",
    "nclass = 5\n",
    "epochList = [1] # default 5\n",
    "lr = 1e-3\n",
    "\n",
    "modelID = 0\n",
    "batchList = [100] # default 10\n",
    "\n",
    "\n",
    "batchSize = batchList[0]\n",
    "epochSize = epochList[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Deep CNN model architecture\n",
    "global inp1\n",
    "\n",
    "def conv_unit(unit, input_layer):\n",
    "    s = '_' + str(unit)\n",
    "    layer = Conv1D(name='Conv1' + s, filters=32, kernel_size=5, strides=1, padding='same', activation='relu')(input_layer)\n",
    "    layer = Conv1D(name='Conv2' + s, filters=32, kernel_size=5, strides=1, padding='same', activation=None)(layer )\n",
    "    layer = Add(name='ResidualSum' + s)([layer, input_layer])\n",
    "    layer = Activation(\"relu\", name='Act' + s)(layer)\n",
    "    layer = MaxPooling1D(name='MaxPool' + s, pool_size=5, strides=2)(layer)\n",
    "    return layer\n",
    "\n",
    "def createModel(shape,lr = 0.001):\n",
    "\n",
    "    ##=============\n",
    "    inp1 = Input(shape=(shape,1),name='inp1')\n",
    "    \n",
    "    #model a1\n",
    "    convLayer = Conv1D(32, kernel_size=5,name='conv1',strides=1)(inp1)\n",
    "    \n",
    "    for i in range(0,5):\n",
    "        convLayer = conv_unit(i+1, convLayer)\n",
    "    ##=============\n",
    "    \n",
    "    FC = Flatten()(convLayer)\n",
    "    FC = Dense(32,activation='relu', name='FC1')(FC)\n",
    "    FC = Dense(nclass,activation=activation_func(3), name='FC_out')(FC)\n",
    "\n",
    "    model = Model(inputs=[inp1], outputs=FC)\n",
    "\n",
    "    model.summary()\n",
    "    model.compile(optimizer=opt_func(1,lr), \n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=[\"acc\"])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset path\n",
    "rootDataFolder = \"kaggle dataset\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# confusion matrix plot\n",
    "def plot_confusion_matrix(y_true, y_pred, classes,\n",
    "                          normalize=False,\n",
    "                          title=None,\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if not title:\n",
    "        if normalize:\n",
    "            title = 'Normalized confusion matrix'\n",
    "        else:\n",
    "            title = 'Confusion matrix, without normalization'\n",
    "\n",
    "    # Compute confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    # Only use the labels that appear in the data\n",
    "    classes = classes[unique_labels(y_true, y_pred)]\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    fig, ax = plt.subplots()\n",
    "    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    ax.figure.colorbar(im, ax=ax)\n",
    "    # We want to show all ticks...\n",
    "    ax.set(xticks=np.arange(cm.shape[1]),\n",
    "           yticks=np.arange(cm.shape[0]),\n",
    "           # ... and label them with the respective list entries\n",
    "           xticklabels=classes, yticklabels=classes,\n",
    "           title=title,\n",
    "           ylabel='True label',\n",
    "           xlabel='Predicted label')\n",
    "\n",
    "    # Rotate the tick labels and set their alignment.\n",
    "    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n",
    "             rotation_mode=\"anchor\")\n",
    "\n",
    "    # Loop over data dimensions and create text annotations.\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            ax.text(j, i, format(cm[i, j], fmt),\n",
    "                    ha=\"center\", va=\"center\",\n",
    "                    color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    fig.tight_layout()\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training model\n",
    "def train_model(xtr,ytr,xtx,ytx,savetype=0):\n",
    "    tf.keras.backend.clear_session()\n",
    "    num_batches_per_epoch = batchSize\n",
    "    epoch = epochSize\n",
    "    cvscores = []\n",
    "\n",
    "    print(\"before SMOTE\")\n",
    "    counter = Counter(ytr)\n",
    "    print(counter)\n",
    "    print(ytr.shape)\n",
    "\n",
    "    # apply synthetic oversampling\n",
    "    oversample = SMOTE()\n",
    "    trX, trY = oversample.fit_resample(xtr, ytr)\n",
    "    \n",
    "#     trX = np.copy(xtr)\n",
    "#     trY = np.copy(ytr)\n",
    "    print(\"after SMOTE\")\n",
    "    counter = Counter(trY)\n",
    "    print(counter)\n",
    "    print(trY.shape)\n",
    "    \n",
    "    trX = trX.reshape(trX.shape[0],trX.shape[1],1)\n",
    "    trY = to_categorical(trY.astype(np.int),nclass)\n",
    "    \n",
    "    testX = xtx.reshape(xtx.shape[0],xtx.shape[1],1)\n",
    "    testY = to_categorical(ytx.astype(np.int),nclass)\n",
    "    \n",
    "    lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "                        initial_learning_rate=1e-3,\n",
    "                        decay_steps=10000,\n",
    "                        decay_rate=0.75)\n",
    "    model = createModel(trX.shape[1],lr_schedule)\n",
    "    \n",
    "    file_path = currentPath+\"/ckpt/\"+\"normal\"\n",
    "        \n",
    "    if savetype == 0:\n",
    "        ckpttype = ['min_loss','max_acc','last']\n",
    "\n",
    "        fpl = list() #file path list\n",
    "        for i in range(0, len(ckpttype)):\n",
    "            fpl.append(file_path+\"_\"+ckpttype[i]+\".h5\")\n",
    "\n",
    "        #print(fpl)\n",
    "\n",
    "        callbacks_list = list()\n",
    "        checkpoint_last = ModelCheckpoint(fpl[2], monitor='acc',period=epoch, verbose=1, save_best_only=False, mode='max')\n",
    "        checkpoint_min_loss = ModelCheckpoint(fpl[0], monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "        checkpoint_max_acc = ModelCheckpoint(fpl[1], monitor='acc', verbose=1, save_best_only=True, mode='max')\n",
    "        callbacks_list = [checkpoint_last,checkpoint_min_loss,checkpoint_max_acc]\n",
    "\n",
    "    Wsave = model.get_weights()\n",
    "\n",
    "    history = model.fit(trX, trY,batch_size=num_batches_per_epoch, epochs=epoch,\n",
    "                        callbacks=callbacks_list, verbose=1,shuffle=True,\n",
    "                        workers=1, use_multiprocessing=False)\n",
    "#         history = model.fit(testX, testY,batch_size=num_batches_per_epoch, epochs=epoch,\n",
    "#                             verbose=1,shuffle=True)\n",
    "\n",
    "    #modelEvalTrain = model.evaluate(trX, trY,batch_size=num_batches_per_epoch, verbose=2)\n",
    "    modelEvalTest = model.evaluate(testX, testY,batch_size=num_batches_per_epoch, verbose=2)\n",
    "\n",
    "    #print(\"Model eval train result : {} - Model eval test result : {}\".format(modelEvalTrain,modelEvalTest))\n",
    "    print(\"Model eval test result : {}\".format(modelEvalTest))\n",
    "    print(\"%s: %.2f%%\" % (model.metrics_names[1], modelEvalTest[1]*100))\n",
    "    cvscores.append(modelEvalTest[1] * 100)\n",
    "\n",
    "    model.set_weights(Wsave)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training checkpoint evaluation\n",
    "def evaluate_model(xtx,ytx):\n",
    "    model = 0\n",
    "    str_result = \"\"\n",
    "    print(ckpt_root_dir)\n",
    "    custom_msg  = model_root_dir[5:]\n",
    "\n",
    "    ckpttype = ['min_loss','max_acc','last']\n",
    "    #ckpttype = ['last']\n",
    "\n",
    "    for i,ckpt_val in enumerate(ckpttype):\n",
    "        cm = list()\n",
    "\n",
    "        tf.keras.backend.clear_session()#clear session\n",
    "\n",
    "        testX = xtx.reshape(xtx.shape[0],xtx.shape[1],1)\n",
    "        testY = to_categorical(ytx.astype(np.int),nclass)\n",
    "\n",
    "        if (model == 0):        \n",
    "            model = createModel(testX.shape[1])\n",
    "            strmodel = get_model_summary(model)\n",
    "            summaryModelPath = currentPath+'/modelSummary/msum_'+model_root_dir+'.txt'\n",
    "            save_modelSummary(strmodel,summaryModelPath)\n",
    "\n",
    "        model.load_weights(ckpt_root_dir+'/'+'normal'+'_'+ckpt_val+'.h5')\n",
    "\n",
    "        #predResult = model.predict_on_batch([testX])\n",
    "        predResult = model.predict([testX])\n",
    "        print(\"predshape {}\".format(predResult.shape))\n",
    "\n",
    "        ytrue = predictionToLabel(testY,nclass)\n",
    "        ypred = predictionToLabel(predResult,nclass)\n",
    "#         print(classification_report(ytrue, ypred, target_names=['1','2','3','4','5']))\n",
    "#         print()\n",
    "        \n",
    "        disp = plot_confusion_matrix(y_true = ytrue,y_pred= ypred,\n",
    "                                     #classes=np.array([0,1,2,3,4]),\n",
    "                                     classes=np.array(['N','S','V','F','Q']),\n",
    "                                 cmap=plt.cm.Blues,\n",
    "                                 normalize=True)\n",
    "        plt.show()\n",
    "        print(confusion_matrix(ytrue,ypred,np.arange(nclass)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## personal note\n",
    "pnote = \"Deep CNN MITBIH training\"\n",
    "pnote = \" \\\\n\"\n",
    "pnote = pnote+\"\"\n",
    "\n",
    "pathNote = data_result_path[0]+\"\\PNOTE.txt\"\n",
    "\n",
    "print(pnote)\n",
    "with open(pathNote,'a') as f:\n",
    "        f.write(pnote)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# laod data\n",
    "xtrain = np.load(rootDataFolder+\"/x_train.npy\")\n",
    "ytrain = np.load(rootDataFolder+\"/y_train.npy\")\n",
    "xtest = np.load(rootDataFolder+\"/x_test.npy\")\n",
    "ytest = np.load(rootDataFolder+\"/y_test.npy\")\n",
    "\n",
    "print(xtrain.shape)\n",
    "print(ytrain.shape)\n",
    "print(xtest.shape)\n",
    "print(ytest.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model(xtrain,ytrain,xtest,ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model(xtest,ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset notebook after trainign to release resource (memory - RAM and GPU)\n",
    "from IPython.core.display import HTML\n",
    "HTML(\"<script>Jupyter.notebook.kernel.restart()</script>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
