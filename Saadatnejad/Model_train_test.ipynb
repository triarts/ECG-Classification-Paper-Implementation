{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paper Implementation\n",
    "Description: This is implementation of paper from S.Saadatnejad et al. \"LSTM-Based ECG Classification for Continuous Monitoring on Personal Wearable Devices\" [(source)](https://ieeexplore.ieee.org/abstract/document/8691755)\n",
    "  * They also have source code and preprocessed dataset for dual lead: http://lis.ee.sharif.edu/pub/2020_jbhi_soh/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO next\n",
    "- [x] implementation for single lead using MITBIH dataset (selfmade implementation preprocessing method based on author paper)\n",
    "- [ ] Add additional explanation and information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\";\n",
    " \n",
    "# The GPU id to use, usually either \"0\" or \"1\";\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\";\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.14.0\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "from glob import glob\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import gc\n",
    "import pickle\n",
    "import pathlib\n",
    "import io\n",
    "from math import sqrt\n",
    "\n",
    "\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint,EarlyStopping,Callback\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.layers import concatenate,Input\n",
    "from tensorflow.keras import optimizers, losses, activations\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "print(tf.__version__)\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.neural_network import MLPClassifier as mlp\n",
    "import joblib as jl #import dump, load\n",
    "from datetime import datetime\n",
    "\n",
    "#from calcCM import *\n",
    "import calcCM as cl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## limit GPU memory usage\n",
    "configproto = tf.compat.v1.ConfigProto()\n",
    "configproto.gpu_options.per_process_gpu_memory_fraction = 0.3\n",
    "configproto.gpu_options.allow_growth = True\n",
    "\n",
    "#limit CPU logical core usage\n",
    "configproto.intra_op_parallelism_threads=2\n",
    "configproto.inter_op_parallelism_threads=2\n",
    "\n",
    "#setup session config\n",
    "sess = tf.compat.v1.Session(config=configproto)\n",
    "tf.compat.v1.keras.backend.set_session(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Arta\\py\\forgit\\saad\\CODE\n"
     ]
    }
   ],
   "source": [
    "dirPath = os.path.abspath(os.getcwd())\n",
    "print(dirPath)\n",
    "#print(pathlib.Path().absolute())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "date and time = 20200908_203123_saad_selfmade\n"
     ]
    }
   ],
   "source": [
    "now = datetime.now()\n",
    "dt_string_daytime = now.strftime(\"%Y%m%d_%H%M%S\")\n",
    "dt_string_daytime +='_saad_selfmade'\n",
    "print(\"date and time =\", dt_string_daytime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "create new folder model_20200908_203123_saad_selfmade\n",
      "create new folder model_20200908_203123_saad_selfmade/ckpt\n",
      "create new folder model_20200908_203123_saad_selfmade/fitHistory\n",
      "create new folder model_20200908_203123_saad_selfmade/modelSummary\n"
     ]
    }
   ],
   "source": [
    "data_result_path = list()\n",
    "data_result_path.append('model_'+dt_string_daytime)\n",
    "data_result_path.append(data_result_path[0]+'/ckpt')\n",
    "data_result_path.append(data_result_path[0]+'/fitHistory')\n",
    "data_result_path.append(data_result_path[0]+'/modelSummary')\n",
    "\n",
    "for i in range(0,len(data_result_path)):\n",
    "    if not os.path.exists(data_result_path[i]):\n",
    "        print(\"create new folder \"+data_result_path[i])\n",
    "        os.makedirs(data_result_path[i])\n",
    "    else:\n",
    "        print(data_result_path[i]+\" already created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Arta\\py\\forgit\\saad\\CODE/model_20200908_203123_saad_selfmade\n"
     ]
    }
   ],
   "source": [
    "currentPath = dirPath+'/'+data_result_path[0]\n",
    "print(currentPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def activation_func(pos):\n",
    "    if(pos== 1):\n",
    "        return activations.relu\n",
    "    elif(pos==2):\n",
    "        return activations.sigmoid\n",
    "    elif(pos==3):\n",
    "        return activations.softmax\n",
    "    elif(pos==4):\n",
    "        return activations.tanh\n",
    "    else:\n",
    "        return activations.relu\n",
    "    \n",
    "def opt_func(pos,lr=0.0001):\n",
    "    if(pos==1):\n",
    "        return optimizers.Adam(learning_rate=lr, beta_1=0.9, beta_2=0.999, amsgrad=False)\n",
    "    elif(pos==2):\n",
    "        return optimizers.SGD(lr=lr, momentum=0.9, decay=0.0, nesterov=False)\n",
    "    elif(pos==3):\n",
    "        return optimizers.Nadam(learning_rate=lr, beta_1=0.9, beta_2=0.999)\n",
    "    elif(pos==4):\n",
    "        return optimizers.Adamax(learning_rate=lr, beta_1=0.9, beta_2=0.999)\n",
    "    else:\n",
    "        return optimizers.Adam(learning_rate=lr, beta_1=0.9, beta_2=0.999, amsgrad=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LSTM hyperparam\n",
    "modelID = 0\n",
    "batchList = [100]\n",
    "epochList = [100]\n",
    "neuronListA = [30];\n",
    "neuronListB = [50];\n",
    "activList = [4]\n",
    "denseActivList = [3]\n",
    "optfList=[1]\n",
    "lr = 1e-3\n",
    "nstepsA = 10\n",
    "nstepsB = 10\n",
    "nstepsC = 5\n",
    "nclass = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setup LSTM model hyperparam\n",
    "neuronA = neuronListA[0]\n",
    "neuronB = neuronListB[0]\n",
    "actf = activList[0]\n",
    "dense_actf = denseActivList[0]\n",
    "optf = optfList[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "global inp1,inp2,inp3,inp4\n",
    "\n",
    "def createModelAlpha(timesteps, nInput):\n",
    "    inp1 = Input(shape=(timesteps[0], nInput[0]),name='inp1')\n",
    "    inp2 = Input(shape=(timesteps[1], nInput[1]),name='inp2')\n",
    "    \n",
    "    \n",
    "    #model a1\n",
    "    #modela1 = LSTM(neuronA, activation=activation_func(actf), name='a1_1')(inp1)\n",
    "    modela1 = LSTM(neuronA, name='a1_1')(inp1)\n",
    "    #modela1 = LSTM(neuronA,activation_func(actf),name='a1_2')(modela1)\n",
    "\n",
    "    #modela2\n",
    "    #modela2 = LSTM(neuronA, activation=activation_func(actf),name='a2_1')(inp2)\n",
    "    modela2 = LSTM(neuronA, name='a2_1')(inp2)\n",
    "    #modela2 = LSTM(neuronA,activation_func(actf),name='a2_2')(modela2)\n",
    "    #modela2 = Flatten()(modela2)\n",
    "\n",
    "    #model a Merge and dense\n",
    "    mergeA = concatenate([modela1,modela2],name='merge_a1a2')\n",
    "    modelAout = Dense(nclass,activation=activation_func(dense_actf), name='modelA_dense')(mergeA)\n",
    "\n",
    "    model = Model(inputs=[inp1, inp2], outputs=modelAout)\n",
    "\n",
    "    model.summary()\n",
    "    model.compile(optimizer=opt_func(optf,lr), \n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=[\"acc\"])\n",
    "    return model\n",
    "\n",
    "def createModelBeta(timesteps, nInput):\n",
    "\n",
    "    ##=============\n",
    "    inp3 = Input(shape=(timesteps[2], nInput[2]),name='inp3')\n",
    "    \n",
    "    #single layer LSTM\n",
    "    ##=============\n",
    "    #model b and dense\n",
    "    #modelb = LSTM(neuronB, activation=activation_func(actf), name='b_1')(inp3)\n",
    "    modelb = LSTM(neuronB, name='b_1')(inp3)\n",
    "    #modelb = LSTM(neuronB,activation_func(actf),name='b_2')(modelb)\n",
    "    modelBout = Dense(nclass,activation=activation_func(dense_actf), name='modelB_dense')(modelb)\n",
    "\n",
    "    model = Model(inputs=[inp3], outputs=modelBout)\n",
    "\n",
    "    model.summary()\n",
    "    model.compile(optimizer=opt_func(optf,lr), \n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=[\"acc\"])\n",
    "    return model\n",
    "\n",
    "def createModelBlend():\n",
    "    inp4 = Input(shape=(14),name='inp4')\n",
    "    \n",
    "    ##=============\n",
    "    #merge ab result\n",
    "    mergeAB = Dense(80,activation=activation_func(dense_actf), name='hidden1')(inp4)\n",
    "    mergeAB = Dense(10,activation=activation_func(dense_actf), name='hidden2')(mergeAB)\n",
    "    modelABout = Dense(nclass,activation=activation_func(dense_actf), name='modelAB_dense')(mergeAB)\n",
    "\n",
    "    model = Model(inputs=[inp4], outputs=modelABout)\n",
    "\n",
    "    model.summary()\n",
    "    model.compile(optimizer=opt_func(optf,lr), \n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=[\"acc\"])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataPadMultiple(sigs,mod):\n",
    "    numpad = mod - (sigs.shape[1]%mod)\n",
    "    \n",
    "    pad = np.zeros((sigs.shape[0],sigs.shape[1]+numpad), dtype=np.float32)\n",
    "    pad[:sigs.shape[0],:sigs.shape[1]] = sigs\n",
    "    \n",
    "    padded_beat = pad\n",
    "    \n",
    "    return padded_beat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "allData = ['100', '101', '103', '105', '106', '107', '108', '109', '111', \n",
    "           '112', '113', '114', '115', '116', '117', '118', '119', '121', '122', '123', '124',\n",
    "           '200', '201', '202', '203', '205', '207', '208', '209', '210',\n",
    "           '212', '213', '214', '215', '217', '219', '220', '221', '222', '223', '228', '230', '231', '232', '233', '234']\n",
    "\n",
    "excludeList = [102,104,107,217]\n",
    "\n",
    "patientIdList = allData\n",
    "\n",
    "rootDataFolder = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def runModified(model_type,model=0,modelAA=0,modelBB=0):\n",
    "    \n",
    "    for idx_ptnId,ptnId in enumerate(patientIdList):\n",
    "\n",
    "        if(int(ptnId) in excludeList):\n",
    "            print(ptnId+' is excluded ###################################')\n",
    "            continue;\n",
    "        print('Load Data -- '+str(ptnId)+'-----------')\n",
    "\n",
    "        # ======================== load data\n",
    "        \n",
    "        # original\n",
    "        trainXa = np.load(rootDataFolder+'/trainingData/'+ptnId+'_A1_saad_MLII_X.npy') #a1\n",
    "        trainXb = np.load(rootDataFolder+'/trainingData/'+ptnId+'_A2_saad_MLII_X.npy') #a2\n",
    "        trainXc = np.load(rootDataFolder+'/trainingData/'+ptnId+'_B_saad_PCA_MLII_X.npy') #b\n",
    "        trainY = np.load(rootDataFolder+'/trainingData/'+ptnId+'_MLII_Y.npy')\n",
    "\n",
    "        testXa = np.load(rootDataFolder+'/testingData/'+ptnId+'_A1_saad_MLII_X.npy') #a1\n",
    "        testXb = np.load(rootDataFolder+'/testingData/'+ptnId+'_A2_saad_MLII_X.npy') #a2\n",
    "        testXc = np.load(rootDataFolder+'/testingData/'+ptnId+'_B_saad_PCA_MLII_X.npy') #b\n",
    "        testY = np.load(rootDataFolder+'/testingData/'+ptnId+'_MLII_Y.npy')\n",
    "        \n",
    "        trXa = trainXa\n",
    "        trXb = trainXb\n",
    "        trXc = trainXc\n",
    "        trY = trainY\n",
    "\n",
    "    # ## checking purpose\n",
    "    #     print(trXa1.shape)\n",
    "    #     print(trXa2.shape)\n",
    "    #     print(trXb.shape)\n",
    "    #     print(trY.shape)\n",
    "\n",
    "    #     print('========')\n",
    "    #     print(testXa1.shape)\n",
    "    #     print(testXa2.shape)\n",
    "    #     print(testXb.shape)\n",
    "    #     print(testY.shape)\n",
    "    \n",
    "        unique, counts = np.unique(trainY, return_counts=True)\n",
    "        print(np.asarray((unique, counts)).T)\n",
    "        print(np.sum(counts))\n",
    "        print()\n",
    "\n",
    "        timesteps_a = nstepsA\n",
    "        timesteps_b = nstepsB\n",
    "        timesteps_c = nstepsC\n",
    "        \n",
    "        trXa = dataPadMultiple(trXa,timesteps_a)\n",
    "        trXb = dataPadMultiple(trXb,timesteps_b)\n",
    "        trXc = dataPadMultiple(trXc,timesteps_c)\n",
    "        \n",
    "        testXa = dataPadMultiple(testXa,timesteps_a)\n",
    "        testXb = dataPadMultiple(testXb,timesteps_b)\n",
    "        testXc = dataPadMultiple(testXc,timesteps_c)\n",
    "\n",
    "        #===================== test data preparation\n",
    "\n",
    "        nInput_a = testXa.shape[1]//timesteps_a\n",
    "        nInput_b = testXb.shape[1]//timesteps_b\n",
    "        nInput_c = testXc.shape[1]//timesteps_c\n",
    "\n",
    "        \n",
    "        #test data model.evaluation\n",
    "        testXa = testXa.reshape(testXa.shape[0],timesteps_a,nInput_a)\n",
    "        testXb = testXb.reshape(testXb.shape[0],timesteps_b,nInput_b)\n",
    "        testXc = testXc.reshape(testXc.shape[0],timesteps_c,nInput_c)\n",
    "        testY = to_categorical(testY.astype(np.int),nclass)\n",
    "        \n",
    "\n",
    "\n",
    "        #================== train data preparation\n",
    "\n",
    "        nInput_a = trXa.shape[1]//timesteps_a\n",
    "        nInput_b = trXb.shape[1]//timesteps_b\n",
    "        nInput_c = trXc.shape[1]//timesteps_c\n",
    "\n",
    "        trXa = trXa.reshape(trXa.shape[0],timesteps_a,nInput_a)\n",
    "        trXb = trXb.reshape(trXb.shape[0],timesteps_b,nInput_b)\n",
    "        trXc = trXc.reshape(trXc.shape[0],timesteps_c,nInput_c)\n",
    "        trY = to_categorical(trY.astype(np.int),nclass)\n",
    "        \n",
    "        #define input model\n",
    "        timestepsList = list()\n",
    "        timestepsList.append(timesteps_a)\n",
    "        timestepsList.append(timesteps_b)\n",
    "        timestepsList.append(timesteps_c)\n",
    "        nInputList = list()\n",
    "        nInputList.append(nInput_a)\n",
    "        nInputList.append(nInput_b)\n",
    "        nInputList.append(nInput_c)\n",
    "\n",
    "        print(trY.shape)\n",
    "\n",
    "        print('######### Training for patient '+str(ptnId)+'--############')\n",
    "\n",
    "        # training duration\n",
    "        batch_size = batchList[0]\n",
    "        num_batches_per_epoch = int(np.ceil(trainY.shape[0]/batch_size))\n",
    "        epoch = epochList[0]\n",
    "        \n",
    "        clear_session = True\n",
    "        if clear_session:\n",
    "            tf.keras.backend.clear_session()\n",
    "        model_code = ['_aa','_bb']\n",
    "        # code_idx => for model checkpoint naming purpose for model alpha and beta\n",
    "        # model_type => for check point naming purpose for model blend\n",
    "        if model == 0:\n",
    "            if(model_type == 1):\n",
    "                code_idx = 0 \n",
    "                model = createModelAlpha(timestepsList,nInputList)\n",
    "            elif(model_type == 2):\n",
    "                code_idx = 1\n",
    "                model = createModelBeta(timestepsList,nInputList)\n",
    "            elif(model_type == 3 or model_type == 4 or model_type == 5):\n",
    "                code_idx = 2\n",
    "                model = createModelBlend()\n",
    "            \n",
    "        \n",
    "        file_path = currentPath+\"/ckpt/\"+str(ptnId)\n",
    "    \n",
    "        if(code_idx <= 1):\n",
    "            flag_model = model_code[code_idx]\n",
    "            ckpttype = ['min_loss'+flag_model,'max_acc'+flag_model,'last'+flag_model]\n",
    "            fpl = list() #file path list\n",
    "            for i in range(0, len(ckpttype)):\n",
    "                fpl.append(file_path+\"_\"+ckpttype[i]+\".h5\")\n",
    "\n",
    "\n",
    "            checkpoint_timed = list()\n",
    "            checkpoint_last = ModelCheckpoint(fpl[2], monitor='acc',period=epoch, verbose=1, save_best_only=False, mode='max')\n",
    "            checkpoint_min_loss = ModelCheckpoint(fpl[0], monitor='loss', verbose=1, save_best_only=True, mode='min')\n",
    "            checkpoint_max_acc = ModelCheckpoint(fpl[1], monitor='acc', verbose=1, save_best_only=True, mode='max')\n",
    "\n",
    "            callbacks_list = [checkpoint_min_loss,checkpoint_max_acc,checkpoint_last]\n",
    "          \n",
    "        Wsave = model.get_weights()\n",
    "        \n",
    "        if(model_type == 1):\n",
    "            ## AA\n",
    "            history = model.fit([trXa,trXb], trY,batch_size=num_batches_per_epoch, epochs=epoch,\n",
    "                                callbacks=callbacks_list, verbose=1,shuffle=True,\n",
    "                                workers=1, use_multiprocessing=False)\n",
    "        elif(model_type == 2):\n",
    "            ##BB\n",
    "            history = model.fit([trXc], trY,batch_size=num_batches_per_epoch, epochs=epoch,\n",
    "                                callbacks=callbacks_list, verbose=1,shuffle=True,\n",
    "                                workers=1, use_multiprocessing=False)\n",
    "        elif(model_type == 3 or model_type == 4 or model_type == 5):\n",
    "            ##blend\n",
    "            # predict each model aplha and mdoel beta then combine the result prediction\n",
    "            model_root_dir = data_result_path[0] # untuk direct root folder\n",
    "            ckpt_root_dir = model_root_dir+'/ckpt'\n",
    "            if modelAA == 0:\n",
    "                modelAA = createModelAlpha(timestepsList,nInputList)\n",
    "            if modelBB == 0:\n",
    "                modelBB = createModelBeta(timestepsList,nInputList)\n",
    "                \n",
    "            if model_type == 3:\n",
    "                modelAA.load_weights(ckpt_root_dir+'/'+str(ptnId)+'_max_acc_aa.h5')\n",
    "                modelBB.load_weights(ckpt_root_dir+'/'+str(ptnId)+'_max_acc_bb.h5')\n",
    "                ckpt_path = file_path+\"_max_acc_last_blend.h5\"\n",
    "            elif model_type == 4:\n",
    "                modelAA.load_weights(ckpt_root_dir+'/'+str(ptnId)+'_min_loss_aa.h5')\n",
    "                modelBB.load_weights(ckpt_root_dir+'/'+str(ptnId)+'_min_loss_bb.h5')\n",
    "                ckpt_path = file_path+\"_min_loss_last_blend.h5\"\n",
    "            elif model_type == 5:\n",
    "                modelAA.load_weights(ckpt_root_dir+'/'+str(ptnId)+'_last_aa.h5')\n",
    "                modelBB.load_weights(ckpt_root_dir+'/'+str(ptnId)+'_last_bb.h5')\n",
    "                ckpt_path = file_path+\"_last_last_blend.h5\"\n",
    "                \n",
    "            print(ckpt_path)\n",
    "            \n",
    "            predResultAA = modelAA.predict_on_batch([trXa,trXb])\n",
    "            predResultBB = modelBB.predict_on_batch([trXc])\n",
    "            \n",
    "            if(np.size(predResultAA,0)!=np.size(predResultBB,0)):\n",
    "                predResultBB = predResultBB[:predResultAA.shape[0],:]\n",
    "#             print(testXa.shape)\n",
    "#             print(testXc.shape)\n",
    "\n",
    "            mergeAABB = np.concatenate((predResultAA,predResultBB),axis=1)\n",
    "            ep = 200 # default 200\n",
    "        \n",
    "            # mlp sklearn ==========================\n",
    "            clf = 0\n",
    "            clf = mlp(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(80,10), random_state = 1)\n",
    "            clf.fit(mergeAABB, trY)\n",
    "            jl.dump(clf, ckpt_path[:-3]+\".jblib\")\n",
    "            \n",
    "\n",
    "        model.set_weights(Wsave)\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## personal note\n",
    "pnote = 'saadajad single lead implmenetation -> code selfmade, data selfmade\\\\n'\n",
    "pathNote = currentPath+\"\\PNOTE.txt\"\n",
    "with open(pathNote,'a') as f:\n",
    "        f.write(pnote)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run training\n",
    "for i in range(1,6):\n",
    "    runModified(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model checkpoint directory\n",
    "\n",
    "#model_root_dir = 'model_20200402_181004_ori_sm_data_ori_selfmade'\n",
    "model_root_dir = data_result_path[0] # untuk direct root folder\n",
    "ckpt_root_dir = model_root_dir+'/ckpt'\n",
    "nclass = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = 0\n",
    "print(\"###### \"+ckpt_root_dir)\n",
    "custom_msg  = model_root_dir[5:]\n",
    "ckpttype = ['last','min_loss','max_acc']\n",
    "#ckpttype = ['min_loss','max_acc']\n",
    "str_result = \"\"\n",
    "modelAA = 0\n",
    "modelBB = 0\n",
    "for i,ckpt_val in enumerate(ckpttype):\n",
    "    cm = list()\n",
    "    for idx_ptnId,ptnId in enumerate(patientIdList):\n",
    "        if(int(ptnId) in excludeList):\n",
    "            print(ptnId+' is excluded ###################################')\n",
    "            continue;\n",
    "        \n",
    "        testXa = np.load(rootDataFolder+'/testingData/'+ptnId+'_A1_saad_MLII_X.npy') #a1\n",
    "        testXb = np.load(rootDataFolder+'/testingData/'+ptnId+'_A2_saad_MLII_X.npy') #a2\n",
    "        testXc = np.load(rootDataFolder+'/testingData/'+ptnId+'_B_saad_PCA_MLII_X.npy') #b\n",
    "        testY = np.load(rootDataFolder+'/testingData/'+ptnId+'_MLII_Y.npy')\n",
    "\n",
    "        timesteps_a = nstepsA\n",
    "        timesteps_b = nstepsB\n",
    "        timesteps_c = nstepsC\n",
    "        \n",
    "        testXa = dataPadMultiple(testXa,timesteps_a)\n",
    "        testXb = dataPadMultiple(testXb,timesteps_b)\n",
    "        testXc = dataPadMultiple(testXc,timesteps_c)\n",
    "\n",
    "        #===================== test data preparation\n",
    "\n",
    "        nInput_a = testXa.shape[1]//timesteps_a\n",
    "        nInput_b = testXb.shape[1]//timesteps_b\n",
    "        nInput_c = testXc.shape[1]//timesteps_c\n",
    "\n",
    "        \n",
    "        #test data model.evaluation\n",
    "        testXa = testXa.reshape(testXa.shape[0],timesteps_a,nInput_a)\n",
    "        testXb = testXb.reshape(testXb.shape[0],timesteps_b,nInput_b)\n",
    "        testXc = testXc.reshape(testXc.shape[0],timesteps_c,nInput_c)\n",
    "        testY = to_categorical(testY.astype(np.int),nclass)\n",
    "    \n",
    "        \n",
    "        \n",
    "        timestepsList = list()\n",
    "        timestepsList.append(timesteps_a)\n",
    "        timestepsList.append(timesteps_b)\n",
    "        timestepsList.append(timesteps_c)\n",
    "        nInputList = list()\n",
    "        nInputList.append(nInput_a)\n",
    "        nInputList.append(nInput_b)\n",
    "        nInputList.append(nInput_c)\n",
    "\n",
    "\n",
    "        if (model == 0):\n",
    "            model = createModelBlend()\n",
    "        \n",
    "        if modelAA == 0:\n",
    "            modelAA = createModelAlpha(timestepsList,nInputList)\n",
    "        if modelBB == 0:\n",
    "            modelBB = createModelBeta(timestepsList,nInputList)\n",
    "\n",
    "        modelAA.load_weights(ckpt_root_dir+'/'+str(ptnId)+'_'+ckpt_val+'_aa.h5')\n",
    "        modelBB.load_weights(ckpt_root_dir+'/'+str(ptnId)+'_'+ckpt_val+'_bb.h5')\n",
    "\n",
    "        predResultAA = modelAA.predict([testXa,testXb])\n",
    "        predResultBB = modelBB.predict([testXc])\n",
    "\n",
    "        if(np.size(predResultAA,0)!=np.size(predResultBB,0)):\n",
    "            predResultBB = predResultBB[:predResultAA.shape[0],:]\n",
    "\n",
    "        \n",
    "        mergeAABB = np.concatenate((predResultAA,predResultBB),axis=1)\n",
    "        \n",
    "        #sklearn mlp predict =================\n",
    "        clf = 0\n",
    "        clf = jl.load(ckpt_root_dir+'/'+str(ptnId)+'_'+ckpt_val+'_last_blend.jblib')\n",
    "        predResult = clf.predict(mergeAABB)\n",
    "        \n",
    "        \n",
    "        print(\"ptnid {} predshape{}\".format(ptnId,predResult.shape))\n",
    "\n",
    "        ytrue = cl.predictionToLabel(testY,nclass)\n",
    "        ypred = cl.predictionToLabel(predResult,nclass)\n",
    "        cm.append( confusion_matrix(ytrue, ypred,np.arange(nclass)) )\n",
    "\n",
    "    cm_final = np.zeros_like(cm[0])\n",
    "    for idx,idxval in enumerate(cm):\n",
    "        cm_final = cm_final+idxval \n",
    "    \n",
    "    print(cm_final)\n",
    "    print()\n",
    "    tmp_str_result, sumResult= cl.cm_code_paper(model_root_dir,cm_final,ckpt_val)\n",
    "    str_result +=tmp_str_result\n",
    "    \n",
    "cl.saveResult(model_root_dir,str_result,custom_msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import HTML\n",
    "HTML(\"<script>Jupyter.notebook.kernel.restart()</script>\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
